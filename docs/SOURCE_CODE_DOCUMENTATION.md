# Source Code Documentation: Voice Hospital Finder Bot

## Table of Contents

1.  [`app.py` - Application Entry Point](#1-apppy---application-entry-point)
2.  [`db/models.py` - Pydantic Data Models](#2-dbmodelspy---pydantic-data-models)
    *   [`LLMResponseModel`](#llmresponsemodel)
    *   [`RAGGroundedResponseModel`](#raggroundedresponsemodel)
    *   [`HospitalFinderState`](#hospitalfinderstate)
    *   [`TTSResponseModel`](#ttsresponsemodel)
3.  [`db/modules/fine_tuner.py` - LLM Fine-Tuning](#3-dbmodulesfine_tunerpy---llm-fine-tuning)
    *   [`fine_tune_insurance_llm`](#fine_tune_insurance_llmdata_path-str--fine_tune_data_path)
4.  [`graphs/graph_tools.py` - Langchain Tools for Graph](#4-graphsgraph_toolspy---langchain-tools-for-graph)
    *   [`transcribe_audio_tool`](#transcribe_audio_toolaudio_path-str-uid-str---dict)
    *   [`recognize_query_tool`](#recognize_query_toolquery_text-str-uid-str-use_llm-bool--true---dict)
    *   [`text_to_speech_tool`](#text_to_speech_tooltext-str-uid-str-output_dir-str--audiosoutput-convert_to_dialogue-bool--false---dict)
    *   [`hospital_lookup_tool`](#hospital_lookup_tool---listdict)
    *   [`hospital_lookup_rag_tool`](#hospital_lookup_rag_tool---tuplelistdict-str)
5.  [`tools/rag_retrieve.py` - Hospital RAG Retriever](#5-toolsrag_retrievepy---hospital-rag-retriever)
    *   [`class HospitalRAGRetriever`](#class-hospitalragretriever)
        *   [`__init__`](#__init__self-vector_db_path_override-str--none)
        *   [`retrieve`](#retrieveself-user_input-dict-extra_results-int--2---listdict)
        *   [`ground_with_qlora`](#ground_with_qloraself-user_query-str-retrieved_hospitals-listdict---str)
        *   [`ground_results`](#ground_resultsself-user_input-dict-retrieved_hospitals-listdict---raggroundedresponsemodel)
    *   [`rag_search_wrapper`](#rag_search_wrapper---tuplelistdict-listdict-str)
6.  [LangGraph Conversational Flow (Conceptual)](#6-langgraph-conversational-flow-conceptual)

## 1. `app.py` - Application Entry Point

The `app.py` script serves as the main entry point for the Voice Hospital Finder Bot. It initializes the core components and starts the conversational flow managed by LangGraph.

**Flow**:
1.  **Initialization**: Displays a startup animation.
2.  **RAG Retriever Setup**: An instance of `HospitalRAGRetriever` from `tools/rag_retrieve.py` is created. This retriever is central to handling hospital data retrieval and LLM-based grounding.
3.  **Global Retriever**: The created `HospitalRAGRetriever` instance is made accessible globally via `set_rag_retriever_instance` from `graphs/graph_tools.py`, allowing other graph tools to utilize it.
4.  **State Management**: A `HospitalFinderState` object, defined in `db/models.py`, is initialized to maintain the conversational context throughout the user interaction.
5.  **Conversational Start**: The `hospital_finder_graph` (presumably defined in `graphs/hospital_graph.py`) is invoked with the initial state, kicking off the multi-turn dialogue.

## 2. `db/models.py` - Pydantic Data Models

These models define the structured data formats used for communication, state management, and validation within the application.

### `LLMResponseModel`

A Pydantic model for parsing and validating structured output from Language Model responses, typically used after a recognition step to interpret user intent and extract entities.

**Fields**:
*   `query` (str): The original query text.
*   `intent` (str): The detected intent of the user's query (e.g., "find_nearest", "find_best", "find_by_insurance", "find_by_hospital"). This guides the subsequent actions in the conversational graph.
*   `location` (Optional[str]): Extracted geographical location relevant to the query.
*   `hospital_names` (Optional[List[str]]): Specific hospital names mentioned by the user.
*   `hospital_type` (List[str]): Desired hospital specialties or types.
*   `insurance` (List[str]): Insurance providers specified by the user.
*   `provider_name` (Optional[str]): Name of a specific insurance provider.
*   `n_hospitals` (Optional[int]): The requested number of hospitals to return, defaulting to 5.
*   `distance_km` (Optional[float]): The maximum search radius in kilometers, defaulting to 30000.

### `RAGGroundedResponseModel`

Represents the structured response generated after the Retrieval Augmented Generation (RAG) process, combining retrieved data with an LLM-generated narrative.

**Fields**:
*   `hospital_ids` (List[int]): A list of integer IDs corresponding to the hospitals identified as most relevant by the RAG process.
*   `dialogue` (str): A natural language dialogue string, generated by the LLM, that provides a human-friendly summary or recommendation based on the retrieved hospitals. This string is then used for text-to-speech conversion.

### `HospitalFinderState`

Manages the comprehensive state of a single conversational session, tracking all relevant information across multiple turns. This model is critical for LangGraph to maintain context.

**Fields**:
*   `uid` (str): A unique identifier for each conversation session.
*   `input_audio_path` (Optional[str]): File path to the raw audio input from the user.
*   `transcription` (Optional[dict]): The result of converting the user's audio input to text.
*   `recognition` (Optional[dict]): The structured data parsed from the transcription (e.g., `LLMResponseModel`'s content).
*   `clarify_user_response_audio_path` (Optional[str]): Path to user audio when clarification is requested by the bot.
*   `clarify_transcription` (Optional[dict]): Transcription of the user's clarification response.
*   `clarify_recognition` (Optional[dict]): Structured data from the clarification transcription.
*   `hospitals_found` (Optional[dict]): A dictionary storing information about hospitals retrieved during the session.
*   `clarify_bot_response_audio_path` (Optional[str]): Path to the bot's audio response when seeking clarification.
*   `turn_count` (int): Counter for the number of turns in the current conversation.
*   `last_question` (Optional[str]): Stores the last question asked by the bot to guide follow-up logic.
*   `final_response` (Optional[dict]): The final structured response provided by the bot for the user's query.
*   `final_response_text` (Optional[str]): The textual content of the final response.
*   `final_response_audio_path` (Optional[str]): File path to the audio version of the final response.
*   `user_wants_exit` (bool): A flag indicating if the user has signaled their intention to end the conversation.

### `TTSResponseModel`

A model to represent the output of Text-to-Speech operations, allowing for potential control over speech characteristics like tone.

**Fields**:
*   `dialogue` (str): The text content to be synthesized into speech.
*   `tone` (Optional[str]): An optional hint for the desired tone of the synthesized speech (e.g., 'friendly', 'informative').

## 3. `db/modules/fine_tuner.py` - LLM Fine-Tuning

This module provides the functionality to fine-tune a pre-trained Large Language Model (LLM) using the QLoRA technique, specifically adapted for the hospital and insurance domain. The fine-tuning process refines the model's ability to generate relevant and accurate responses based on domain-specific data.

### `fine_tune_insurance_llm(data_path: str)`

This asynchronously callable function orchestrates the entire QLoRA fine-tuning workflow for a causal language model. It's designed to be called during initial setup or when new fine-tuning data becomes available.

**Parameters**:
*   `data_path` (str): The file path to a JSON dataset containing instruction-response pairs specifically prepared for fine-tuning the LLM in the context of hospital and insurance queries.

**Operational Flow**:
1.  **Data Loading & Formatting**: Reads the JSON dataset and processes it into a format suitable for the LLM (instruction-context-response templates).
2.  **Model & Tokenizer Initialization**: Loads a base LLM (e.g., `openlm-research/open_llama_3b_v2`) and its corresponding tokenizer. Quantization (4-bit) is applied using `BitsAndBytesConfig` to reduce memory footprint, a key aspect of QLoRA.
3.  **PEFT Integration**: The loaded base model is prepared for k-bit training and enhanced with LoRA (Low-Rank Adaptation) adapters using the PEFT (Parameter-Efficient Fine-Tuning) library. These adapters are strategically applied to target modules (e.g., attention projections) to enable efficient fine-tuning without modifying the entire model.
4.  **Dataset Tokenization**: The pre-processed and formatted dataset is tokenized into input IDs and attention masks, truncated and padded to a maximum sequence length.
5.  **Training Configuration**: `transformers.TrainingArguments` are configured, specifying parameters like batch size, learning rate, number of epochs, gradient accumulation steps, and logging/saving frequency.
6.  **Training Execution**: A `transformers.Trainer` instance is created and initiates the fine-tuning process.
7.  **Model Saving**: Upon completion, the fine-tuned model (specifically the LoRA adapters) and tokenizer are saved to a specified output directory, ready for deployment.

## 4. `graphs/graph_tools.py` - Langchain Tools for Graph

This module defines a set of Langchain `tool`s. These tools are asynchronous functions wrapped to be callable nodes within the LangGraph conversational flow, enabling the bot to perform specific actions like speech processing, NLU, and database lookups.

### `transcribe_audio_tool(audio_path: str, uid: str) -> dict`

An asynchronous tool that converts spoken audio into text. It integrates with an external transcription service (e.g., OpenAI Whisper via `tools.transcribe.transcribe_wrapper`).

**Parameters**:
*   `audio_path` (str): The file path of the audio recording to be transcribed.
*   `uid` (str): A unique identifier for the current user session, used for logging and tracking.

**Returns**:
*   `dict`: Contains the session `uid`, the original `audio_path`, and the `transcribed_text`.

### `recognize_query_tool(query_text: str, uid: str, use_llm: bool = True) -> dict`

An asynchronous tool responsible for Named Entity Recognition (NER) and intent classification from a textual query. It delegates to `tools.recognize.recognize_wrapper` and can optionally leverage an LLM for more sophisticated understanding.

**Parameters**:
*   `query_text` (str): The text input (e.g., transcribed audio) to be analyzed.
*   `uid` (str): Unique session identifier.
*   `use_llm` (bool): A flag indicating whether to use an advanced LLM for recognition in addition to rule-based or statistical methods.

**Returns**:
*   `dict`: A structured dictionary derived from `LLMResponseModel`, containing the `query`, detected `intent`, and extracted entities like `location`, `hospital_type`, and `insurance`.

### `text_to_speech_tool(text: str, uid: str, output_dir: str = "audios/output", convert_to_dialogue: bool = False) -> dict`

An asynchronous tool that converts a given text string into an audio file, suitable for voice responses. It utilizes `tools.text_to_speech.text_to_speech_wrapper` and saves the output to a specified directory.

**Parameters**:
*   `text` (str): The dialogue text to be converted to speech.
*   `uid` (str): Unique session identifier.
*   `output_dir` (str): The directory where the generated audio file will be saved.
*   `convert_to_dialogue` (bool): A boolean flag, potentially used to trigger additional processing for dialogue-specific speech synthesis.

**Returns**:
*   `dict`: Includes the session `uid`, the original `text`, and the `audio_path` of the generated speech file.

### `hospital_lookup_tool(...) -> List[dict]`

An asynchronous tool for performing a direct lookup of hospitals from the database based on geographical coordinates, intent, hospital types, and insurance providers. This tool calls `tools.hospital_lookup.hospital_lookup_wrapper`.

**Parameters**:
*   `user_lat` (float): Latitude of the user's current or specified location.
*   `user_lon` (float): Longitude of the user's current or specified location.
*   `intent` (str): The user's primary goal (e.g., "find_nearest", "find_best", "find_all_in_radius").
*   `hospital_types` (Optional[List[str]]): A list of required hospital specialties.
*   `insurance_providers` (Optional[List[str]]): A list of accepted insurance providers for filtering.
*   `n_hospitals` (int): The desired number of top hospitals to retrieve.
*   `distance_km_radius` (float): The maximum search radius from the user's location.

**Returns**:
*   `List[dict]`: A list of dictionaries, each representing a hospital that matches the criteria.

### `hospital_lookup_rag_tool(...) -> Tuple[List[dict], str]`

An asynchronous tool that performs a Retrieval Augmented Generation (RAG) based search for hospitals. It leverages a FAISS vector database for semantic search and an LLM for grounding the retrieved results into a conversational dialogue. This tool depends on a pre-initialized `HospitalRAGRetriever` instance.

**Parameters**:
*   `intent` (str): The primary search intent (e.g., "find_nearest", "find_by_hospital").
*   `user_query` (Optional[str]): The original natural language query from the user.
*   `user_loc` (Optional[str]): User's location name.
*   `user_lat` (Optional[float]): User's latitude.
*   `user_lon` (Optional[float]): User's longitude.
*   `hospital_types` (Optional[List[str]]): Desired hospital specialties.
*   `hospital_names` (Optional[List[str]]): Specific hospital names to look for.
*   `insurance_providers` (Optional[List[str]]): Accepted insurance providers for filtering.
*   `n_hospitals` (int): Number of top hospitals to return.
*   `distance_km_radius` (float): Maximum search radius.
*   `extra_results` (int): Additional hospitals to retrieve from FAISS to provide richer context for LLM grounding.

**Returns**:
*   `Tuple[List[dict], str]`: A tuple where the first element is a list of hospital dictionaries (retrieved and selected), and the second is a natural language `dialogue` string generated by the LLM.

## 5. `tools/rag_retrieve.py` - Hospital RAG Retriever

This module encapsulates the core logic for Retrieval Augmented Generation (RAG) in the context of hospital search. It manages the interaction with the FAISS vector database for efficient retrieval and an LLM for contextual grounding of the search results.

### `class HospitalRAGRetriever`

The `HospitalRAGRetriever` class is responsible for loading the hospital vector database, managing the (optional) fine-tuned LLM, performing semantic searches, and grounding the results into human-readable responses.

#### `__init__(self, vector_db_path_override: str = None)`

Initializes the `HospitalRAGRetriever` by setting up the embedding model, loading the FAISS vector database from a specified path, and conditionally loading a fine-tuned LLM if the `GROUND_WITH_FINE_TUNE` configuration flag is enabled.

**Parameters**:
*   `vector_db_path_override` (str, optional): An optional path to override the default FAISS vector database location.

**Internal Operations**:
*   `_load_vector_db()`: Handles the loading of the FAISS vector database using `FAISS.load_local`.
*   `_load_finetuned_model()`: Loads a PEFT-wrapped fine-tuned model and its tokenizer (e.g., from a QLoRA training) to enable specialized response generation for certain intents.
*   `_haversine_distance()`: A static utility method for calculating geographical distances, used for filtering hospitals by radius.
*   `_build_query()`: A static utility method that constructs a natural language query string based on the user's structured input, optimizing it for semantic search in the vector database.

#### `retrieve(self, user_input: Dict, extra_results: int = 2) -> List[Dict]`

Executes a similarity search against the FAISS vector database using a query derived from `user_input`. The results are then filtered by geographical distance and sorted based on the user's specified `intent` (e.g., "find_nearest" prioritizes distance, "find_best" prioritizes rating).

**Parameters**:
*   `user_input` (Dict): A dictionary containing various search parameters from the user's query, such as geographical coordinates (`user_lat`, `user_lon`), desired number of hospitals (`n_hospitals`), search radius (`distance_km_radius`), `intent`, hospital names, types, and insurance providers.
*   `extra_results` (int): An additional number of results to fetch from the vector database beyond `n_hospitals`. These extra results provide more context for the subsequent LLM grounding step, potentially leading to more robust responses.

**Returns**:
*   `List[Dict]`: A sorted list of dictionaries, where each dictionary represents a hospital and includes calculated fields like `distance_km`.

#### `ground_with_qlora(self, user_query: str, retrieved_hospitals: List[dict]) -> str`

Generates a concise, natural language response by leveraging the fine-tuned QLoRA model. This method is specifically employed when `GROUND_WITH_FINE_TUNE` is active and the intent is to address specific hospital inquiries (e.g., "find_by_hospital"). It constructs a prompt by combining the user's query with the details of the retrieved hospitals and generates a contextualized response.

**Parameters**:
*   `user_query` (str): The user's original query text.
*   `retrieved_hospitals` (List[dict]): A list of dictionaries, each detailing a hospital already identified as relevant by the `retrieve` method.

**Returns**:
*   `str`: An LLM-generated dialogue string that directly addresses the user's query using the context of the provided hospitals.

#### `ground_results(self, user_input: dict, retrieved_hospitals: List[dict]) -> RAGGroundedResponseModel`

This is the primary method for grounding retrieved hospital data into a conversational response. It intelligently decides whether to use the specialized fine-tuned QLoRA model (if configured and appropriate for the intent) or a general-purpose LLM to generate the final dialogue. The method constructs a detailed prompt for the chosen LLM, incorporating user input and the context of the retrieved hospitals.

**Parameters**:
*   `user_input` (dict): The complete set of user query parameters.
*   `retrieved_hospitals` (List[dict]): The list of hospitals that were initially retrieved from the vector database.

**Returns**:
*   `RAGGroundedResponseModel`: A Pydantic model containing the IDs of the hospitals that should be highlighted in the response and the generated natural language dialogue.

### `rag_search_wrapper(retriever: HospitalRAGRetriever, ...)`

An asynchronous wrapper function that orchestrates the entire RAG search process. It takes user input, performs retrieval, and then grounds the results, returning both the structured hospital data and an LLM-generated dialogue.

**Parameters**:
*   `retriever` (HospitalRAGRetriever): An initialized instance of the `HospitalRAGRetriever`.
*   Additional parameters are the same as those for `hospital_lookup_rag_tool`, covering user query details, location, intent, and filtering criteria.

**Returns**:
*   `Tuple[List[dict], List[dict], str]`: A tuple containing:
    *   The complete list of hospitals retrieved by the vector database search.
    *   A refined list of hospitals selected after the grounding process.
    *   The LLM-generated natural language dialogue string.

## 6. LangGraph Conversational Flow (Conceptual)

The `hospital_finder_graph` (defined in `graphs/hospital_graph.py`, though not directly reviewed here) is the central orchestration mechanism for the bot's multi-turn conversations. It uses the LangGraph framework to manage state transitions and tool invocations.

**High-Level Flow**:

1.  **Initial Input**: The process begins with the user's audio input, which updates the `HospitalFinderState`.
2.  **Transcription**: The `transcribe_audio_tool` (from `graphs/graph_tools.py`) is called to convert the audio to text.
3.  **Query Recognition**: The `recognize_query_tool` analyzes the transcribed text to extract user intent (`LLMResponseModel`) and entities, updating the state.
4.  **Intent Routing**: Based on the extracted `intent`, the graph routes the flow.
    *   If a hospital search is needed (e.g., "find_nearest", "find_by_hospital"), the `hospital_lookup_rag_tool` is invoked. This tool internally uses the `HospitalRAGRetriever` to:
        *   Perform a semantic search in the FAISS vector database.
        *   Ground the retrieved results using an LLM (potentially fine-tuned) to generate relevant dialogue (`RAGGroundedResponseModel`).
    *   Other intents might lead to different tools or clarification steps.
5.  **Clarification/Response Generation**: The graph might enter a clarification loop if more information is needed from the user. Otherwise, it prepares a final response.
6.  **Text-to-Speech**: The `text_to_speech_tool` converts the generated dialogue into audio, completing the conversational turn.
7.  **State Update**: The `HospitalFinderState` is continuously updated at each step, persisting the conversation's context.

This modular structure allows for flexible and robust conversational management, dynamically invoking the appropriate tools and LLM functionalities based on the evolving dialogue.
